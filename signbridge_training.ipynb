{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5d615b",
   "metadata": {},
   "source": [
    "Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55022e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83e2ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands           # Hands model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94cb2bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"Processes a frame using the MediaPipe model (Hands).\"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # BGR -> RGB\n",
    "    image.flags.writeable = False                  \n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # RGB -> BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5695404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    \"\"\"Draws detected hand landmarks and connections.\"\"\"\n",
    "    \n",
    "    # Check if any hands were detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw landmarks and connections for the current hand\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, \n",
    "                hand_landmarks, \n",
    "                mp_hands.HAND_CONNECTIONS, # Uses mp_hands connections\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4), \n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d7c7b",
   "metadata": {},
   "source": [
    "Extract Keypoints Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Extracts x, y, z coordinates for the 21 landmarks of the FIRST detected hand \n",
    "    and flattens them into a 63-value NumPy array (21 * 3).\n",
    "    \n",
    "    Returns an array of zeros (63,) if no hand is detected.\n",
    "    \"\"\"\n",
    "    \n",
    "    #placeholder\n",
    "    keypoints = np.zeros(21 * 3) \n",
    "    \n",
    "    # The Hands model returns results in 'results.multi_hand_landmarks'.\n",
    "    if results.multi_hand_landmarks:\n",
    "        # Focus on the FIRST detected hand\n",
    "        hand = results.multi_hand_landmarks[0]\n",
    "        \n",
    "        # Extract x, y, z for all 21 landmarks and flatten into a (63,) array.\n",
    "        keypoints = np.array([[res.x, res.y, res.z] for res in hand.landmark]).flatten()\n",
    "        \n",
    "    return keypoints\n",
    "\n",
    "# keypoint_vector = extract_keypoints(results)\n",
    "# print(keypoint_vector.shape) # Output should be (63,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# max_num_hands=2 allowed\n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5, max_num_hands=2) as hands:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Make detections (passing the 'hands' model)\n",
    "        image, results = mediapipe_detection(frame, hands)\n",
    "        \n",
    "        # Draw landmarks (Hand-specific drawing)\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        #Extract and get the feature vector\n",
    "        keypoint_vector = extract_keypoints(results)\n",
    "\n",
    "        # if not np.all(keypoint_vector == 0):\n",
    "        #     print(\"Hand Detected! Vector length:\", keypoint_vector.shape)\n",
    "        #     print(\"Sample Values:\", keypoint_vector[0:6]) # Print the first 6 values\n",
    "        # else:\n",
    "        #     print(\"No Hand Detected (Vector is all zeros)\")\n",
    "\n",
    "        cv2.putText(image, f'Vector Size: {keypoint_vector.shape}', (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('SignBridge Hand Detection Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Clean up\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25e7914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_styled_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9b4a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6874f7",
   "metadata": {},
   "source": [
    "Setup Folder for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data')\n",
    "\n",
    "# 29 Common Indian Standard Hand Signs (ISL)\n",
    "actions = np.array([\n",
    "    'hello', 'thanks', 'sorry', 'please', 'yes', 'no',    # Greetings/Manners\n",
    "    'I', 'you', 'name', 'time', 'what', 'where', 'how',  # Questions/Pronouns\n",
    "    'help', 'learn', 'work', 'eat', 'drink', 'home',     # Actions/Places\n",
    "    'good', 'bad', 'happy', 'sad', 'tired',              # Feelings\n",
    "    'one', 'two', 'three', 'four', 'five'                # Numbers\n",
    "])\n",
    "\n",
    "# # Sixty sequences worth of data \n",
    "no_sequences = 60\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "#Folder start\n",
    "start_folder = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    for sequence in range(1, no_sequences + 1):\n",
    "        try: \n",
    "            # This creates paths like: 'MP_Data/hello/1', 'MP_Data/thanks/2', etc.\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66c6be",
   "metadata": {},
   "source": [
    "Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c7baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data collection complete.\n"
     ]
    }
   ],
   "source": [
    "break_flag = False\n",
    "\n",
    "os.makedirs(DATA_PATH, exist_ok=True) \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands \n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5, max_num_hands=2) as hands:\n",
    "     \n",
    "    # Loop through actions (30 ISL Signs)\n",
    "    for action in actions:\n",
    "        # CHECK 1: Exit action loop if flag is set\n",
    "        if break_flag:\n",
    "            break\n",
    "            \n",
    "        # Loop through sequences aka videos (60 samples per sign)\n",
    "        for sequence in range(start_folder, start_folder + no_sequences):\n",
    "            # CHECK 2: Exit sequence loop if flag is set\n",
    "            if break_flag:\n",
    "                break\n",
    "            \n",
    "            # Loop through video length aka sequence length (30 frames per sample)\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "\n",
    "                # Make detections (Using the 'hands' model)\n",
    "                image, results = mediapipe_detection(frame, hands) \n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                if frame_num == 0: \n",
    "                    # Display instruction\n",
    "                    cv2.putText(image, 'SIGN: ' + action.upper(), (30, 80), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'PRESS SPACE TO RECORD', (30, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                    cv2.imshow('SignBridge Data Collection Feed', image)\n",
    "                    \n",
    "                    while True:\n",
    "                        key = cv2.waitKey(1)\n",
    "                        if key & 0xFF == 32: # Spacebar pressed\n",
    "                            break\n",
    "                        if key & 0xFF == ord('q'):\n",
    "                            # SET THE BREAK FLAG HERE (no 'global' needed as it's top-level)\n",
    "                            break_flag = True\n",
    "                            break # Break out of the inner while loop\n",
    "                \n",
    "                else: \n",
    "                    # Display recording status\n",
    "                    cv2.putText(image, 'SIGN: ' + action.upper(), (30, 80), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 255), 5, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'RECORDING... Frame: {}'.format(frame_num), (30, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('SignBridge Data Collection Feed', image)\n",
    "                \n",
    "                # Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                \n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break_flag = True\n",
    "                    break \n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if break_flag:\n",
    "    print(\"\\nData collection manually stopped by user ('q'). Clean exit from all loops.\")\n",
    "else:\n",
    "    print(\"\\nData collection complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "196e3f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.20976734e-01  5.06322384e-01  3.54597745e-07  8.64707053e-01\n",
      "  4.61435795e-01  7.49870948e-03  8.83628249e-01  4.18436944e-01\n",
      "  7.98900519e-03  8.97339821e-01  3.80015910e-01  8.63042381e-03\n",
      "  9.19853449e-01  3.61854196e-01  8.57773237e-03  8.40409338e-01\n",
      "  3.32743913e-01 -1.69777423e-02  8.60230863e-01  2.54025221e-01\n",
      " -2.75644213e-02  8.70371759e-01  2.04063684e-01 -3.19840387e-02\n",
      "  8.77809882e-01  1.68231025e-01 -3.44632827e-02  8.04935217e-01\n",
      "  3.32628727e-01 -2.34557949e-02  8.05117965e-01  2.36434609e-01\n",
      " -3.58894616e-02  8.03960145e-01  1.79003596e-01 -3.99411842e-02\n",
      "  8.02050591e-01  1.36247873e-01 -4.21160460e-02  7.74348438e-01\n",
      "  3.48875552e-01 -2.80257612e-02  7.58534431e-01  2.66039610e-01\n",
      " -4.09392491e-02  7.51441061e-01  2.13719666e-01 -5.12330979e-02\n",
      "  7.47850955e-01  1.73928082e-01 -5.71116917e-02  7.50176966e-01\n",
      "  3.76037627e-01 -3.02906446e-02  7.17127979e-01  3.24305683e-01\n",
      " -4.55108434e-02  6.96071565e-01  2.91274041e-01 -5.59962019e-02\n",
      "  6.80379152e-01  2.61501431e-01 -6.13622442e-02]\n"
     ]
    }
   ],
   "source": [
    "print(np.load('MP_Data/five/60/29.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b16e228",
   "metadata": {},
   "source": [
    "Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33b3512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4227497",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06a52861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0,\n",
       " 'thanks': 1,\n",
       " 'sorry': 2,\n",
       " 'please': 3,\n",
       " 'yes': 4,\n",
       " 'no': 5,\n",
       " 'I': 6,\n",
       " 'you': 7,\n",
       " 'name': 8,\n",
       " 'time': 9,\n",
       " 'what': 10,\n",
       " 'where': 11,\n",
       " 'how': 12,\n",
       " 'help': 13,\n",
       " 'learn': 14,\n",
       " 'work': 15,\n",
       " 'eat': 16,\n",
       " 'drink': 17,\n",
       " 'home': 18,\n",
       " 'good': 19,\n",
       " 'bad': 20,\n",
       " 'happy': 21,\n",
       " 'sad': 22,\n",
       " 'tired': 23,\n",
       " 'one': 24,\n",
       " 'two': 25,\n",
       " 'three': 26,\n",
       " 'four': 27,\n",
       " 'five': 28}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757edd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        is_valid_sequence = True # New flag to check for bad data\n",
    "        for frame_num in range(sequence_length):\n",
    "            try:\n",
    "                res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "                \n",
    "                # If the entire frame is all zeros, skip the whole sequence\n",
    "                if not np.any(res):\n",
    "                    is_valid_sequence = False\n",
    "                    break \n",
    "                    \n",
    "                window.append(res)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {action}/{sequence}/{frame_num}. Skipping sequence.\")\n",
    "                is_valid_sequence = False\n",
    "                break\n",
    "        \n",
    "        # Only append the window and label if all 30 frames were loaded and valid\n",
    "        if is_valid_sequence:\n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b64518c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1638, 30, 63)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0901f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83b07424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1638, 30, 63)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1e1306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24a62bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Structure Summary ---\n",
      "Total Sequences Loaded: 1638\n",
      "X_train Shape (Features): (1310, 30, 63)\n",
      "y_train Shape (Labels): (1310, 29)\n",
      "X_test Shape (Features): (328, 30, 63)\n",
      "y_test Shape (Labels): (328, 29)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Added random_state for reproducibility\n",
    "\n",
    "print(\"--- Data Structure Summary ---\")\n",
    "print(f\"Total Sequences Loaded: {len(X)}\")\n",
    "print(f\"X_train Shape (Features): {X_train.shape}\")\n",
    "print(f\"y_train Shape (Labels): {y_train.shape}\")\n",
    "print(f\"X_test Shape (Features): {X_test.shape}\")\n",
    "print(f\"y_test Shape (Labels): {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1facf90f",
   "metadata": {},
   "source": [
    "Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3abb5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7a709aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1]   # 30 (time steps)\n",
    "n_features = X_train.shape[2]        # 63 (keypoints features)\n",
    "n_classes = y_train.shape[1]         # 29 (number of signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454bda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1: LSTM Input\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(sequence_length, n_features)))\n",
    "model.add(Dropout(0.2)) # Added Dropout for robustness\n",
    "\n",
    "# Layer 2: Second LSTM Layer\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "\n",
    "# Layer 3: Final LSTM Layer (return_sequences=False to output a single vector)\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dropout(0.2)) # Added Dropout for robustness\n",
    "\n",
    "# Layer 4: Dense Classification Head\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Layer 5: Output Layer (CRITICAL CHANGE: Output units must be n_classes = 29)\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e12584c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02bdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training (200 Epochs) ---\n",
      "Epoch 1/200\n",
      "41/41 [==============================] - 7s 43ms/step - loss: 3.3676 - categorical_accuracy: 0.0275\n",
      "Epoch 2/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 3.2739 - categorical_accuracy: 0.0489\n",
      "Epoch 3/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 2.9036 - categorical_accuracy: 0.0885\n",
      "Epoch 4/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 2.6238 - categorical_accuracy: 0.1290\n",
      "Epoch 5/200\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 2.4963 - categorical_accuracy: 0.1481\n",
      "Epoch 6/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 2.2910 - categorical_accuracy: 0.1992\n",
      "Epoch 7/200\n",
      "41/41 [==============================] - 2s 50ms/step - loss: 2.1048 - categorical_accuracy: 0.2397\n",
      "Epoch 8/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 2.0089 - categorical_accuracy: 0.2954\n",
      "Epoch 9/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 1.8912 - categorical_accuracy: 0.3206\n",
      "Epoch 10/200\n",
      "41/41 [==============================] - 2s 44ms/step - loss: 1.8219 - categorical_accuracy: 0.3634\n",
      "Epoch 11/200\n",
      "41/41 [==============================] - 2s 43ms/step - loss: 1.5220 - categorical_accuracy: 0.4267\n",
      "Epoch 12/200\n",
      "41/41 [==============================] - 2s 43ms/step - loss: 3.2841 - categorical_accuracy: 0.2603\n",
      "Epoch 13/200\n",
      "41/41 [==============================] - 1s 34ms/step - loss: 1.8733 - categorical_accuracy: 0.3573\n",
      "Epoch 14/200\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 1.6045 - categorical_accuracy: 0.4168\n",
      "Epoch 15/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 1.4256 - categorical_accuracy: 0.4763\n",
      "Epoch 16/200\n",
      "41/41 [==============================] - 2s 44ms/step - loss: 1.3505 - categorical_accuracy: 0.5038\n",
      "Epoch 17/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 1.3190 - categorical_accuracy: 0.5183\n",
      "Epoch 18/200\n",
      "41/41 [==============================] - 2s 51ms/step - loss: 1.2820 - categorical_accuracy: 0.5305\n",
      "Epoch 19/200\n",
      "41/41 [==============================] - 2s 49ms/step - loss: 1.2217 - categorical_accuracy: 0.5282\n",
      "Epoch 20/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 1.1809 - categorical_accuracy: 0.5664\n",
      "Epoch 21/200\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 1.2716 - categorical_accuracy: 0.5435\n",
      "Epoch 22/200\n",
      "41/41 [==============================] - 2s 46ms/step - loss: 1.2226 - categorical_accuracy: 0.5725\n",
      "Epoch 23/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 1.6168 - categorical_accuracy: 0.4321\n",
      "Epoch 24/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 1.3365 - categorical_accuracy: 0.5008\n",
      "Epoch 25/200\n",
      "41/41 [==============================] - 2s 46ms/step - loss: 1.1510 - categorical_accuracy: 0.5779\n",
      "Epoch 26/200\n",
      "41/41 [==============================] - 2s 52ms/step - loss: 1.1182 - categorical_accuracy: 0.5802\n",
      "Epoch 27/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 1.0181 - categorical_accuracy: 0.6275\n",
      "Epoch 28/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 1.0538 - categorical_accuracy: 0.6206\n",
      "Epoch 29/200\n",
      "41/41 [==============================] - 2s 44ms/step - loss: 0.8826 - categorical_accuracy: 0.6611\n",
      "Epoch 30/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 1.1884 - categorical_accuracy: 0.5725\n",
      "Epoch 31/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.8683 - categorical_accuracy: 0.6710\n",
      "Epoch 32/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.7931 - categorical_accuracy: 0.7137\n",
      "Epoch 33/200\n",
      "41/41 [==============================] - 2s 44ms/step - loss: 1.3754 - categorical_accuracy: 0.5489\n",
      "Epoch 34/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 1.0160 - categorical_accuracy: 0.6519\n",
      "Epoch 35/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.9487 - categorical_accuracy: 0.6374\n",
      "Epoch 36/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.8186 - categorical_accuracy: 0.6977\n",
      "Epoch 37/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.9414 - categorical_accuracy: 0.6855\n",
      "Epoch 38/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.9750 - categorical_accuracy: 0.6443\n",
      "Epoch 39/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.7131 - categorical_accuracy: 0.7313\n",
      "Epoch 40/200\n",
      "41/41 [==============================] - 2s 43ms/step - loss: 0.7117 - categorical_accuracy: 0.7504\n",
      "Epoch 41/200\n",
      "41/41 [==============================] - 2s 44ms/step - loss: 0.8017 - categorical_accuracy: 0.7023\n",
      "Epoch 42/200\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 0.6734 - categorical_accuracy: 0.7450\n",
      "Epoch 43/200\n",
      "41/41 [==============================] - 2s 37ms/step - loss: 0.6419 - categorical_accuracy: 0.7672\n",
      "Epoch 44/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.5991 - categorical_accuracy: 0.7992\n",
      "Epoch 45/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.6306 - categorical_accuracy: 0.7679\n",
      "Epoch 46/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.6937 - categorical_accuracy: 0.7527\n",
      "Epoch 47/200\n",
      "41/41 [==============================] - 2s 46ms/step - loss: 0.7457 - categorical_accuracy: 0.7382\n",
      "Epoch 48/200\n",
      "41/41 [==============================] - 2s 49ms/step - loss: 0.5853 - categorical_accuracy: 0.7939\n",
      "Epoch 49/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.7175 - categorical_accuracy: 0.7595\n",
      "Epoch 50/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.5603 - categorical_accuracy: 0.8008\n",
      "Epoch 51/200\n",
      "41/41 [==============================] - 2s 49ms/step - loss: 0.5374 - categorical_accuracy: 0.8214\n",
      "Epoch 52/200\n",
      "41/41 [==============================] - 2s 43ms/step - loss: 0.4405 - categorical_accuracy: 0.8481\n",
      "Epoch 53/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.6216 - categorical_accuracy: 0.7908\n",
      "Epoch 54/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.3844 - categorical_accuracy: 0.8580\n",
      "Epoch 55/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.3815 - categorical_accuracy: 0.8656\n",
      "Epoch 56/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.4710 - categorical_accuracy: 0.8412\n",
      "Epoch 57/200\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 0.3656 - categorical_accuracy: 0.8802\n",
      "Epoch 58/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.6486 - categorical_accuracy: 0.7885\n",
      "Epoch 59/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.3630 - categorical_accuracy: 0.8786\n",
      "Epoch 60/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.3748 - categorical_accuracy: 0.8733\n",
      "Epoch 61/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.4083 - categorical_accuracy: 0.8695\n",
      "Epoch 62/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.3105 - categorical_accuracy: 0.8893\n",
      "Epoch 63/200\n",
      "41/41 [==============================] - 2s 43ms/step - loss: 0.3367 - categorical_accuracy: 0.8863\n",
      "Epoch 64/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.3083 - categorical_accuracy: 0.8908\n",
      "Epoch 65/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.3399 - categorical_accuracy: 0.8870\n",
      "Epoch 66/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.3630 - categorical_accuracy: 0.8771\n",
      "Epoch 67/200\n",
      "41/41 [==============================] - 2s 44ms/step - loss: 0.3464 - categorical_accuracy: 0.8885\n",
      "Epoch 68/200\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 0.2988 - categorical_accuracy: 0.8985\n",
      "Epoch 69/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.3232 - categorical_accuracy: 0.9031\n",
      "Epoch 70/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.3959 - categorical_accuracy: 0.8740\n",
      "Epoch 71/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.2435 - categorical_accuracy: 0.9275\n",
      "Epoch 72/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.1918 - categorical_accuracy: 0.9405\n",
      "Epoch 73/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.3129 - categorical_accuracy: 0.9061\n",
      "Epoch 74/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.3704 - categorical_accuracy: 0.8916\n",
      "Epoch 75/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.2476 - categorical_accuracy: 0.9214\n",
      "Epoch 76/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.2893 - categorical_accuracy: 0.9153\n",
      "Epoch 77/200\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 0.2506 - categorical_accuracy: 0.9099\n",
      "Epoch 78/200\n",
      "41/41 [==============================] - 2s 45ms/step - loss: 0.8965 - categorical_accuracy: 0.7656\n",
      "Epoch 79/200\n",
      "41/41 [==============================] - 2s 45ms/step - loss: 0.7517 - categorical_accuracy: 0.7779\n",
      "Epoch 80/200\n",
      "41/41 [==============================] - 2s 43ms/step - loss: 0.4151 - categorical_accuracy: 0.8863\n",
      "Epoch 81/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.2786 - categorical_accuracy: 0.9160\n",
      "Epoch 82/200\n",
      "41/41 [==============================] - 2s 47ms/step - loss: 0.2483 - categorical_accuracy: 0.9229\n",
      "Epoch 83/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.4519 - categorical_accuracy: 0.8733\n",
      "Epoch 84/200\n",
      "41/41 [==============================] - 2s 37ms/step - loss: 0.2043 - categorical_accuracy: 0.9427\n",
      "Epoch 85/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.1601 - categorical_accuracy: 0.9550\n",
      "Epoch 86/200\n",
      "41/41 [==============================] - 2s 44ms/step - loss: 0.1888 - categorical_accuracy: 0.9481\n",
      "Epoch 87/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.2770 - categorical_accuracy: 0.9084\n",
      "Epoch 88/200\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 0.1779 - categorical_accuracy: 0.9397\n",
      "Epoch 89/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.1696 - categorical_accuracy: 0.9557\n",
      "Epoch 90/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.3745 - categorical_accuracy: 0.8901\n",
      "Epoch 91/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.2213 - categorical_accuracy: 0.9298\n",
      "Epoch 92/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.1658 - categorical_accuracy: 0.9504\n",
      "Epoch 93/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.1526 - categorical_accuracy: 0.9588\n",
      "Epoch 94/200\n",
      "41/41 [==============================] - 2s 44ms/step - loss: 0.1356 - categorical_accuracy: 0.9573\n",
      "Epoch 95/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.2581 - categorical_accuracy: 0.9160\n",
      "Epoch 96/200\n",
      "41/41 [==============================] - 2s 37ms/step - loss: 0.1534 - categorical_accuracy: 0.9573\n",
      "Epoch 97/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.2127 - categorical_accuracy: 0.9397\n",
      "Epoch 98/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.3862 - categorical_accuracy: 0.8977\n",
      "Epoch 99/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.3022 - categorical_accuracy: 0.9046\n",
      "Epoch 100/200\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 0.2959 - categorical_accuracy: 0.9176\n",
      "Epoch 101/200\n",
      "41/41 [==============================] - 2s 44ms/step - loss: 0.1374 - categorical_accuracy: 0.9641\n",
      "Epoch 102/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.1553 - categorical_accuracy: 0.9550\n",
      "Epoch 103/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.1507 - categorical_accuracy: 0.9519\n",
      "Epoch 104/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.2905 - categorical_accuracy: 0.9237\n",
      "Epoch 105/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.2474 - categorical_accuracy: 0.9153\n",
      "Epoch 106/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.1718 - categorical_accuracy: 0.9527\n",
      "Epoch 107/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.1449 - categorical_accuracy: 0.9557\n",
      "Epoch 108/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.0945 - categorical_accuracy: 0.9725\n",
      "Epoch 109/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.1140 - categorical_accuracy: 0.9611\n",
      "Epoch 110/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.4392 - categorical_accuracy: 0.8824\n",
      "Epoch 111/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.2293 - categorical_accuracy: 0.9328\n",
      "Epoch 112/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.1429 - categorical_accuracy: 0.9550\n",
      "Epoch 113/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.1829 - categorical_accuracy: 0.9450\n",
      "Epoch 114/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.3568 - categorical_accuracy: 0.8969\n",
      "Epoch 115/200\n",
      "41/41 [==============================] - 2s 37ms/step - loss: 0.1551 - categorical_accuracy: 0.9527\n",
      "Epoch 116/200\n",
      "41/41 [==============================] - 2s 45ms/step - loss: 0.1353 - categorical_accuracy: 0.9611\n",
      "Epoch 117/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.1437 - categorical_accuracy: 0.9588\n",
      "Epoch 118/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.2920 - categorical_accuracy: 0.9168\n",
      "Epoch 119/200\n",
      "41/41 [==============================] - 2s 47ms/step - loss: 0.2734 - categorical_accuracy: 0.9206\n",
      "Epoch 120/200\n",
      "41/41 [==============================] - 2s 52ms/step - loss: 0.2244 - categorical_accuracy: 0.9359\n",
      "Epoch 121/200\n",
      "41/41 [==============================] - 2s 50ms/step - loss: 0.1705 - categorical_accuracy: 0.9450\n",
      "Epoch 122/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.1076 - categorical_accuracy: 0.9718\n",
      "Epoch 123/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.4126 - categorical_accuracy: 0.8962\n",
      "Epoch 124/200\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 0.2182 - categorical_accuracy: 0.9427\n",
      "Epoch 125/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.1491 - categorical_accuracy: 0.9573\n",
      "Epoch 126/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.1345 - categorical_accuracy: 0.9565\n",
      "Epoch 127/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.1620 - categorical_accuracy: 0.9626\n",
      "Epoch 128/200\n",
      "41/41 [==============================] - 1s 34ms/step - loss: 0.2094 - categorical_accuracy: 0.9366\n",
      "Epoch 129/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.2184 - categorical_accuracy: 0.9282\n",
      "Epoch 130/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.1242 - categorical_accuracy: 0.9679\n",
      "Epoch 131/200\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 0.1276 - categorical_accuracy: 0.9595\n",
      "Epoch 132/200\n",
      "41/41 [==============================] - 1s 34ms/step - loss: 0.1314 - categorical_accuracy: 0.9603\n",
      "Epoch 133/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.4237 - categorical_accuracy: 0.8870\n",
      "Epoch 134/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.1453 - categorical_accuracy: 0.9595\n",
      "Epoch 135/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.0905 - categorical_accuracy: 0.9687\n",
      "Epoch 136/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.0857 - categorical_accuracy: 0.9779\n",
      "Epoch 137/200\n",
      "41/41 [==============================] - 2s 37ms/step - loss: 0.1903 - categorical_accuracy: 0.9511\n",
      "Epoch 138/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.0867 - categorical_accuracy: 0.9786\n",
      "Epoch 139/200\n",
      "41/41 [==============================] - 2s 37ms/step - loss: 0.1406 - categorical_accuracy: 0.9557\n",
      "Epoch 140/200\n",
      "41/41 [==============================] - 1s 37ms/step - loss: 0.0759 - categorical_accuracy: 0.9763\n",
      "Epoch 141/200\n",
      "41/41 [==============================] - 2s 43ms/step - loss: 0.1008 - categorical_accuracy: 0.9748\n",
      "Epoch 142/200\n",
      "41/41 [==============================] - 2s 37ms/step - loss: 0.0847 - categorical_accuracy: 0.9763\n",
      "Epoch 143/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.0624 - categorical_accuracy: 0.9832\n",
      "Epoch 144/200\n",
      "41/41 [==============================] - 1s 34ms/step - loss: 0.0519 - categorical_accuracy: 0.9863\n",
      "Epoch 145/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.0463 - categorical_accuracy: 0.9870\n",
      "Epoch 146/200\n",
      "41/41 [==============================] - 2s 50ms/step - loss: 1.0779 - categorical_accuracy: 0.7359\n",
      "Epoch 147/200\n",
      "41/41 [==============================] - 2s 46ms/step - loss: 0.2821 - categorical_accuracy: 0.9183\n",
      "Epoch 148/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.1364 - categorical_accuracy: 0.9573\n",
      "Epoch 149/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.1921 - categorical_accuracy: 0.9534\n",
      "Epoch 150/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.2347 - categorical_accuracy: 0.9359\n",
      "Epoch 151/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.1152 - categorical_accuracy: 0.9634\n",
      "Epoch 152/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.1204 - categorical_accuracy: 0.9740\n",
      "Epoch 153/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.1242 - categorical_accuracy: 0.9603\n",
      "Epoch 154/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.0887 - categorical_accuracy: 0.9802\n",
      "Epoch 155/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.1022 - categorical_accuracy: 0.9679\n",
      "Epoch 156/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.1098 - categorical_accuracy: 0.9695\n",
      "Epoch 157/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.3860 - categorical_accuracy: 0.9038\n",
      "Epoch 158/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.1836 - categorical_accuracy: 0.9504\n",
      "Epoch 159/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.1053 - categorical_accuracy: 0.9695\n",
      "Epoch 160/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.1027 - categorical_accuracy: 0.9710\n",
      "Epoch 161/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.0908 - categorical_accuracy: 0.9756\n",
      "Epoch 162/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.1028 - categorical_accuracy: 0.9687\n",
      "Epoch 163/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.3852 - categorical_accuracy: 0.9084\n",
      "Epoch 164/200\n",
      "41/41 [==============================] - 2s 43ms/step - loss: 0.0898 - categorical_accuracy: 0.9786\n",
      "Epoch 165/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.0882 - categorical_accuracy: 0.9725\n",
      "Epoch 166/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.0862 - categorical_accuracy: 0.9771\n",
      "Epoch 167/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.1267 - categorical_accuracy: 0.9626\n",
      "Epoch 168/200\n",
      "41/41 [==============================] - 2s 45ms/step - loss: 0.0960 - categorical_accuracy: 0.9725\n",
      "Epoch 169/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.0710 - categorical_accuracy: 0.9802\n",
      "Epoch 170/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.0978 - categorical_accuracy: 0.9779\n",
      "Epoch 171/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.1032 - categorical_accuracy: 0.9649\n",
      "Epoch 172/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.0617 - categorical_accuracy: 0.9802\n",
      "Epoch 173/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.0583 - categorical_accuracy: 0.9832\n",
      "Epoch 174/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.0577 - categorical_accuracy: 0.9847\n",
      "Epoch 175/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.1139 - categorical_accuracy: 0.9664\n",
      "Epoch 176/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.1196 - categorical_accuracy: 0.9595\n",
      "Epoch 177/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.0778 - categorical_accuracy: 0.9779\n",
      "Epoch 178/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.0698 - categorical_accuracy: 0.9840\n",
      "Epoch 179/200\n",
      "41/41 [==============================] - 2s 45ms/step - loss: 0.1076 - categorical_accuracy: 0.9672\n",
      "Epoch 180/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.1199 - categorical_accuracy: 0.9664\n",
      "Epoch 181/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.0802 - categorical_accuracy: 0.9771\n",
      "Epoch 182/200\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 0.2635 - categorical_accuracy: 0.9366\n",
      "Epoch 183/200\n",
      "41/41 [==============================] - 2s 44ms/step - loss: 0.1939 - categorical_accuracy: 0.9511\n",
      "Epoch 184/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.1293 - categorical_accuracy: 0.9618\n",
      "Epoch 185/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.0615 - categorical_accuracy: 0.9824\n",
      "Epoch 186/200\n",
      "41/41 [==============================] - 2s 42ms/step - loss: 0.0642 - categorical_accuracy: 0.9794\n",
      "Epoch 187/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.0753 - categorical_accuracy: 0.9802\n",
      "Epoch 188/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.0647 - categorical_accuracy: 0.9779\n",
      "Epoch 189/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.0684 - categorical_accuracy: 0.9802\n",
      "Epoch 190/200\n",
      "41/41 [==============================] - 2s 45ms/step - loss: 0.0629 - categorical_accuracy: 0.9832\n",
      "Epoch 191/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.0715 - categorical_accuracy: 0.9771\n",
      "Epoch 192/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.0700 - categorical_accuracy: 0.9817\n",
      "Epoch 193/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.3028 - categorical_accuracy: 0.9221\n",
      "Epoch 194/200\n",
      "41/41 [==============================] - 2s 43ms/step - loss: 0.2315 - categorical_accuracy: 0.9397\n",
      "Epoch 195/200\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.2261 - categorical_accuracy: 0.9374\n",
      "Epoch 196/200\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 0.1050 - categorical_accuracy: 0.9725\n",
      "Epoch 197/200\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 0.0833 - categorical_accuracy: 0.9786\n",
      "Epoch 198/200\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 0.0610 - categorical_accuracy: 0.9832\n",
      "Epoch 199/200\n",
      "41/41 [==============================] - 2s 37ms/step - loss: 0.0341 - categorical_accuracy: 0.9931\n",
      "Epoch 200/200\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 0.0569 - categorical_accuracy: 0.9855\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Model Training (200 Epochs) ---\")\n",
    "history = model.fit(X_train, y_train, epochs=200, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1a55335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 30, 64)            32768     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 29)                957       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 188189 (735.11 KB)\n",
      "Trainable params: 188189 (735.11 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4814418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2b7c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Predicted Sign: **one**\n"
     ]
    }
   ],
   "source": [
    "sample_index = 4\n",
    "\n",
    "#Print the Model's Prediction\n",
    "model_prediction_index = np.argmax(res[sample_index])\n",
    "predicted_sign = actions[model_prediction_index]\n",
    "\n",
    "print(f\"Model Predicted Sign: **{predicted_sign}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72509d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Correct Sign:    **one**\n"
     ]
    }
   ],
   "source": [
    "#Print the True Label\n",
    "true_label_index = np.argmax(y_test[sample_index])\n",
    "true_sign = actions[true_label_index]\n",
    "\n",
    "print(f\"True Correct Sign:    **{true_sign}**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f6d5a",
   "metadata": {},
   "source": [
    "Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21a86f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = model.predict(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31cc616",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1973ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.argmax(yhat_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c862e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_accuracy = accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cae36bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns 29 separate 2x2 matrices\n",
    "cm = multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e1bfb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Model Evaluation ---\n",
      "Overall Test Accuracy: 97.87%\n",
      "\n",
      "--- Confusion Matrix Summary (First 5 Signs) ---\n",
      "Sign: HELLO\n",
      "            Pred NEG  Pred POS\n",
      "Actual NEG       318         0\n",
      "Actual POS         0        10\n",
      "--------------------\n",
      "Sign: THANKS\n",
      "            Pred NEG  Pred POS\n",
      "Actual NEG       318         0\n",
      "Actual POS         0        10\n",
      "--------------------\n",
      "Sign: SORRY\n",
      "            Pred NEG  Pred POS\n",
      "Actual NEG       318         0\n",
      "Actual POS         0        10\n",
      "--------------------\n",
      "Sign: PLEASE\n",
      "            Pred NEG  Pred POS\n",
      "Actual NEG       315         0\n",
      "Actual POS         0        13\n",
      "--------------------\n",
      "Sign: YES\n",
      "            Pred NEG  Pred POS\n",
      "Actual NEG       312         0\n",
      "Actual POS         0        16\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# 5. Display Results Clearly\n",
    "print('\\n--- Final Model Evaluation ---')\n",
    "print(f\"Overall Test Accuracy: {overall_accuracy * 100:.2f}%\")\n",
    "\n",
    "print('\\n--- Confusion Matrix Summary (First 5 Signs) ---')\n",
    "# We print the first 5 matrices for inspection.\n",
    "# Each matrix is [TN, FP] and [FN, TP] for that class vs. all others.\n",
    "for i in range(min(5, len(actions))):\n",
    "    print(f\"Sign: {actions[i].upper()}\")\n",
    "    # Using pandas DataFrame for clean display\n",
    "    df_cm = pd.DataFrame(cm[i], index=['Actual NEG', 'Actual POS'], columns=['Pred NEG', 'Pred POS'])\n",
    "    print(df_cm)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546a78f",
   "metadata": {},
   "source": [
    "Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fef48cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PBL\\SignBridge\\myenv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('signbridge_isl_29_signs.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
